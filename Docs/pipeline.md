# Generative AI pipeline

The Generative AI pipeline consists of 
* Data acquistion -> Collect the data
* Data Preparation -> Cleaning the data
* Feature Engineering 
* Modelling 
* Evaluation -> Evaluating the model
* Deployment -> Hosting the model
* Monitoring and Model Updating

## Data Acquistion
Available Data -> CSV,TEXT,PDF,DOCS,XLSX
Other Data (Data base,internet,API,Web Scrapping)
No data (Create your own data) or Use LLM to generate the data

Data augmentation : If you have less data then you can perform data agumentation
Exp :- 1) Replace with synonyms
I am a data scientist
-> I am an Ai engineer

Biagram Fill : I am C
-> C is my name 

Basically increasing the data here

Back Translate
Converting from one language to another 
And from that another language to english

This way we have new data

## Add additional data/noise
I am a data scientist, i like this job

'i like this job' -> Is noise

## Image Augmentation
Image augmentation is a technique that creates new training data from existing images to increase the diversity of the data set.


## Data Preprocessing

1) Here we can remove tags,remove emoji,spelling check
2) Basic Preprocessing
3) Advanced Application

In Basic Preprocessing we have

Tokenization -> Sentence and word

Optional Preprocessing -> 
1) Stop word removal 
2) Stemming -> Less used
3) Lamatization -> More used
4) Punctuation -> Removal (/,-,$)
5) Lower case
6) Language Detection

Tokenization -> Taking the words individually 
Tokenization is the process of dividing a text into smaller units known as tokens. Tokens are typically words or sub-words in the context of natural language processing. 

