{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyStemmer in c:\\users\\chitresh.laptop-dad6s1e5\\appdata\\roaming\\python\\python312\\site-packages (2.2.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\CHITRESH.LAPTOP-\n",
      "[nltk_data]     DAD6S1E5\\AppData\\Roaming\\nltk_data...DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\chitresh.laptop-dad6s1e5\\appdata\\roaming\\python\\python312\\site-packages (2.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330[nltk_data] Downloading package wordnet to C:\\Users\\CHITRESH.LAPTOP-\n",
      "[nltk_data]     DAD6S1E5\\AppData\\Roaming\\nltk_data...\n",
      "\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "!pip install PyStemmer\n",
    "from stemming.lovins import stem\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install emoji\n",
    "import emoji\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of tokenization\n",
    "Sentence Level tokenization\n",
    "Word level tokenization (Mosty useds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone .', 'Today we are learning GenAI .', 'Hope You enjoy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello everyone . Today we are learning GenAI . Hope You enjoy\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'this', 'is', 'chitresh']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtext = \"hello this is chitresh\"\n",
    "word_tokenize(wordtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming -> This reduces the word to their root form \n",
    "\n",
    "Let's consider the words -> Dancing,Danced,Dancer,Dances \n",
    "\n",
    "This all gets converted to Dance\n",
    "\n",
    "This is less used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem_text = \"Love looks not with the eyes but with the mind, and therefore is winged Cupid painted blind.\"\n",
    "words = word_tokenize(stem_text)\n",
    "## loop through the sentence \n",
    "stemmed_words = [stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemmed_text = \"Love looks not with the eyes but with the mind, and therefore is winged Cupid painted blind.\"\n",
    "words = word_tokenize(stemmed_text)\n",
    "stemmed_words = [stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowerCasing \n",
    "\n",
    "Consistency: Applying lowercase conversion before tokenization ensures that all tokens are in the same case, reducing the variability caused by case differences.\n",
    "\r\n",
    "Simplicity: It simplifies the tokenization process because the tokenizer doesn‚Äôt need to handle case sensitivity, making the tokens more uniform\n",
    ".\r\n",
    "Efficiency: Lowercasing the entire text at once can be more efficient than converting each token separately after tokenizatio\n",
    "\n",
    "Named Entity Recognition (NER): Case sensitivity can be crucial for distinguishing between entities.\n",
    "\r\n",
    "Machine Translation: Preserving case can be important for proper nouns and acronyms.\r\n",
    "Language Models: For models that need to understand nuanced differences between cases, like differentiating ‚ÄúUS‚Äù (United States) from ‚Äúus‚Äù (pronoun).n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', '.', 'today', 'we', 'are', 'learning', 'genai', '.', 'hope', 'you', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "normal_text = \"Natural Language PreProcessing is Fun\"\n",
    "lower_text = text.lower()\n",
    "check_lower = word_tokenize(lower_text)\n",
    "print(check_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Tags\n",
    "\n",
    "This ensures only the relevant text is represented and only meaningful textual content is analyzed\n",
    "\n",
    "emoving HTML tags helps to extract pure textual data, making it easier to focus on the actual content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important function\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # important syntax\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "tag_text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
    "\n",
    "remove_html_tags(tag_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Urls\n",
    "\n",
    "Removing URLs is essential to eliminate irrelevant information that doesn't contribute to linguistic analysis.\n",
    "\n",
    "This is pretty self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check out chat gpt here \n",
      "check out perplexity here  \n",
      "check out claude here \n"
     ]
    }
   ],
   "source": [
    "# function to remove URL's\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "text1 = \"check out chat gpt here https://chatgpt.com/\"\n",
    "text2 = \"check out perplexity here https://www.perplexity.ai/ \"\n",
    "text3 = \"check out claude here https://claude.ai/\"\n",
    "\n",
    "print(remove_url(text1))\n",
    "print(remove_url(text2))\n",
    "print(remove_url(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuations\n",
    "\n",
    "Removing punctuation marks is essential in NLP text preprocessing to enhance the accuracy and efficiency of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog However the dog doesnt seem impressed Oh no it just yawned How disappointing Maybe a squirrel would elicit a reaction Alas the fox is out of luck'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', punc))\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\"\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling ChatWords (Important)\n",
    "\n",
    "Handling ChatWords, also known as internet slang or informal language used in online communication, is important in NLP text preprocessing to ensure accurate analysis and understanding of text data. By converting ChatWords into their standard English equivalents or formal language equivalents, NLP models can effectively interpret the meaning of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I faced an error here where i noticed after a several tries that the data is in key and value pairs\n",
    "chat_words = {\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "     \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "     \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words: #upper returns where all the characters are in upper case\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "        return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion\n",
      "For Your Information\n"
     ]
    }
   ],
   "source": [
    "text = 'IMHO he is the best'\n",
    "text1 = 'FYI Islamabad is the capital of Pakistan'\n",
    "\n",
    "print(chat_conversion(text))\n",
    "print(chat_conversion(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic -> Meaning the right interpretation of words\n",
    "\n",
    "In NLP text preprocessing, removing stop words is crucial to enhance the quality and efficiency of analysis. Stop words are common words like \"the,\" \"is,\" and \"and,\" which appear frequently in text but carry little semantic meaning.\n",
    "\n",
    "\n",
    " This process streamlines the analysis by focusing on the significant words that carry more meaningful information, leading to better model performance and interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "# .split -> Splits the string into a list\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = [] #creates a new list\n",
    "    \n",
    "    for word in text.split(): #iterates through the list after splitting into a list\n",
    "        if word in stopword:\n",
    "            new_text.append('') # if it's a stop word adds and empty string\n",
    "        else:\n",
    "            new_text.append(word) # otherwise it adds the word\n",
    "    x = new_text[:] #reates a shallow copy of the new_text list and assigns it to a new variable x. The slice [:] takes all elements of new_text.\n",
    "    new_text.clear() # Clears all elements from the new_text list. After this operation, new_text becomes an empty list.\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "\n",
    "print(remove_stopwords(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Emojis\n",
    "Handling emojis in NLP text preprocessing is essential for several reasons. Emojis convey valuable information about sentiment, emotion, and context in text data, especially in informal communication channels like social media. However, they pose challenges for NLP algorithms due to their non-textual nature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was üòò \n",
      " Python is üî•\n",
      "Loved the movie. It was \n",
      "Loved the movie. It was :face_blowing_a_kiss:\n",
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "text = \"Loved the movie. It was üòò\"\n",
    "text1 = 'Python is üî•'\n",
    "print(text ,'\\n', text1)\n",
    "\n",
    "# Remove Emojies using Fucntion\n",
    "print(remove_emoji(text))\n",
    "remove_emoji(text1)\n",
    "\n",
    "## use this -> demojize\n",
    "print(emoji.demojize(text))\n",
    "print(emoji.demojize(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization -> This is better than stemming this ensures that the word is reduced to it's actual base form \n",
    "\n",
    "Whereas stemming is faster but less accurate, producing root forms that may not be real words. it only removes common endings like -ing, -ed, -ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# we use the nltk library\n",
    "\n",
    "wordnet_lemmatizer =  WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "punctuations=\"?:!.,;\" #puncatuations to be removed\n",
    " \n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words: # for removing punctuations\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word) #Removes the punctuation mark from the sentence_words list if it's found.\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\")) # Formats the column headers \"Word\" and \"Lemma\" into two columns of width 20 characters each.\n",
    "\n",
    "for word in sentence_words: #iterates\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n",
    "\n",
    "# POS V specifies the verb \n",
    "# This ensures that verbs are lemmatized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
